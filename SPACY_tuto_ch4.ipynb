{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eda8a20",
   "metadata": {},
   "source": [
    "### Creating training data(1)\n",
    "\n",
    "spaCy's rule-based `Matcher` is a great way to quickly create training data for named entity models. A list of sentences is available as the variable `TEXTS`. You can print it to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as `\"GADGET\"`.\n",
    "\n",
    "- Write a pattern for two tokens whose lowercase forms match `\"iphone\"` and `\"x\"`.\n",
    "- Write a pattern for two tokens: one token whose lowercase form matches `\"iphone\"` and a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e593ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "matcher.add works a bit different at v3\n",
    "'''\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and check the result\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([doc[start:end] for match_id, start, end in matcher(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8e001",
   "metadata": {},
   "source": [
    "### Creating training data(2)\n",
    "\n",
    "Let's use the match patterns we've created in the previous exercise to bootstrap a set of training examples. A list of sentences is available as the variable `TEXTS`.\n",
    "\n",
    "- Create a doc object for each text using `nlp.pipe`.\n",
    "- Match on the `doc` and create a list of matched spans.\n",
    "- Get `(start character, end character, label)` tuples of matched spans.\n",
    "- Format each example as a tuple of the text and a dict, mapping `\"entities\"` to the entity tuples.\n",
    "- Append the example to `TRAINING_DATA` and inspect the printed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a9d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "(\"iPhone 11 vs iPhone 8: What's the difference?\", {'entities': [(0, 9, 'GADGET'), (13, 21, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "matcher.add works a bit different at v3\n",
    "'''\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac523c",
   "metadata": {},
   "source": [
    "### Setting up the pipeline\n",
    "\n",
    "In this exercise, you'll prepare a spaCy pipeline to train the entity recognizer to recognize `\"GADGET\"` entities in a text â€“ for example, \"iPhone X\".\n",
    "\n",
    "- Create a blank `\"en\"` model, for example using the `spacy.blank` method.\n",
    "- Create a new entity recognizer using `nlp.create_pipe` and add it to the pipeline.\n",
    "- Add the new label `\"GADGET\"` to the entity recognizer using the add_label method on the pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494c0140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank \"en\" model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "nlp.create_pipe('ner')\n",
    "nlp.add_pipe('ner')\n",
    "\n",
    "# Add the label \"GADGET\" to the entity recognizer\n",
    "ner.add_label(\"GADGET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c591ce5",
   "metadata": {},
   "source": [
    "### Building a training loop\n",
    "\n",
    "Let's write a simple training loop from scratch!  \n",
    "\n",
    "The pipeline you've created in the previous exercise is available as the nlp object.  \n",
    "It already contains the entity recognizer with the added label `\"GADGET\".`  \n",
    "  \n",
    "The small set of labelled examples that you've created previously is available as `TRAINING_DATA`.  \n",
    "\n",
    "To see the examples, you can print them in your script.  \n",
    "\n",
    "- Call `nlp.begin_training`, create a training loop for 10 iterations and shuffle the training data.\n",
    "- Create batches of training data using `spacy.util.minibatch` and iterate over the batches.\n",
    "- Convert the `(text, annotations)` tuples in the batch to lists of `texts` and `annotations`.\n",
    "- For each batch, use `nlp.update` to update the model with the texts and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1890b5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 8.158883213996887}\n",
      "{'ner': 17.940181970596313}\n",
      "{'ner': 30.74606227874756}\n",
      "{'ner': 6.24123278260231}\n",
      "{'ner': 14.7780322432518}\n",
      "{'ner': 20.45983499288559}\n",
      "{'ner': 3.5592862367630005}\n",
      "{'ner': 6.439008187502623}\n",
      "{'ner': 10.109526328742504}\n",
      "{'ner': 1.6003090951126069}\n",
      "{'ner': 4.297520248335786}\n",
      "{'ner': 7.065472565176606}\n",
      "{'ner': 0.9593647629226325}\n",
      "{'ner': 3.129635869915546}\n",
      "{'ner': 5.524009907433992}\n",
      "{'ner': 5.002823667789926}\n",
      "{'ner': 12.618434515912668}\n",
      "{'ner': 15.255126489033046}\n",
      "{'ner': 2.2171880868263543}\n",
      "{'ner': 3.3403736823629515}\n",
      "{'ner': 4.743925209504141}\n",
      "{'ner': 0.9311934591439694}\n",
      "{'ner': 2.411232209146192}\n",
      "{'ner': 2.4181894513918323}\n",
      "{'ner': 0.010205586965110758}\n",
      "{'ner': 0.013129191760111514}\n",
      "{'ner': 1.4251981868387849}\n",
      "{'ner': 1.8457801859176162}\n",
      "{'ner': 1.8459988337280244}\n",
      "{'ner': 1.8460088892541209}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "v2 and v3 is totailly different. tutorial about this is based on the v2 which is quite useless.\n",
    "'''\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "from spacy.training import Example\n",
    "\n",
    "with open(\"gadgets.json\", encoding=\"utf8\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "# ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe('ner')\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update([example], losses=losses, drop=0.3)\n",
    "        print(losses)\n",
    "        \n",
    "'''\n",
    "v2\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"exercises/en/gadgets.json\", encoding=\"utf8\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    print(losses)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44e015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
