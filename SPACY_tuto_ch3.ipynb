{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1290ff4",
   "metadata": {},
   "source": [
    "### Inspecting the pipeline\n",
    "\n",
    "**Let's inspect the small English model's pipeline!**\n",
    "\n",
    "- **Load the `en_core_web_sm` model and create the `nlp` object.**\n",
    "- **Print the names of the pipeline components using `nlp.pipe_names`.**\n",
    "- **Print the full pipeline of `(name, component)` tuples using `nlp.pipeline`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0bab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001E9F2BE05E0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001E9F2911720>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001E9F2B993A0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001E9F2BF5280>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001E9F2A25880>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001E9F2A02FA0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f7acd",
   "metadata": {},
   "source": [
    "### Simple components\n",
    "\n",
    "**The example shows a custom component that prints the number of tokens in a document. Can you complete it?**\n",
    "\n",
    "- **Complete the component function with the `doc's` length.**\n",
    "- **Add the `length_component` to the existing pipeline as the first component.**\n",
    "- **Try out the new pipeline and process any text with the `nlp` object â€“ for example \"This is a sentence.\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7079280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tutorial is based on v2 and current is v3. this part seems to be different from the tutorial sample codes\n",
    "'''\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc2c78",
   "metadata": {},
   "source": [
    "### Complex components\n",
    "\n",
    "In this exercise, you'll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable `matcher`.\n",
    "\n",
    "- Define the custom component and apply the `matcher` to the `doc`.\n",
    "- Create a `Span` for each match, assign the label ID for `\"ANIMAL\"` and overwrite the `doc.ents` with the new spans.\n",
    "- Add the new component to the pipeline *after* the `\"ner\"` component.\n",
    "- Process the text and print the entity text and entity label for the entities in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fd0cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tutorial is based on v2 and current is v3. this part seems to be different from the tutorial sample codes\n",
    "'''\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3c659",
   "metadata": {},
   "source": [
    "### Setting extension attributes (1)\n",
    "\n",
    "Let's practice setting some extension attributes.\n",
    "\n",
    "Step 1\n",
    "Use `Token.set_extension` to register `\"is_country\"` (default `False`).\n",
    "Update it for `\"Spain\"` and print it for all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3002b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c1796",
   "metadata": {},
   "source": [
    "Step 2\n",
    "- Use `Token.set_extension` to register \"reversed\" (getter function `get_reversed`).\n",
    "- Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694c5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873e1ab",
   "metadata": {},
   "source": [
    "### Setting extension attributes(2)\n",
    "\n",
    "Let's try setting some more complex attributes using getters and method extensions.\n",
    "\n",
    "Part 1\n",
    "- Complete the `get_has_number` function .\n",
    "- Use `Doc.set_extension` to register `\"has_number\"` (getter `get_has_number`) and print its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce52ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790e9df",
   "metadata": {},
   "source": [
    "Part 2\n",
    "- Use Span.set_extension to register `\"to_html\"` (method `to_html`).\n",
    "- Call it on `doc[0:2]` with the tag `\"strong\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8441c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48b91f",
   "metadata": {},
   "source": [
    "### Entities and extensions\n",
    "\n",
    "In this exercise, you'll combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
    "\n",
    "- Complete the `get_wikipedia_url` getter so it only returns the URL if the spanâ€™s label is in the list of labels.\n",
    "- Set the `Span` extension `\"wikipedia_url\"` using the getter `get_wikipedia_url`.\n",
    "- Iterate over the entities in the `doc` and output their Wikipedia URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91ee282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4bd8b",
   "metadata": {},
   "source": [
    "### Components with extensions\n",
    "\n",
    "Extension attributes are especially powerful if they're combined with custom pipeline components. In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available.\n",
    "\n",
    "A phrase matcher with all countries is available as the variable `matcher`. A dictionary of countries mapped to their capital cities is available as the variable `CAPITALS`.\n",
    "\n",
    "Complete the `countries_component` and create a `Span` with the label `\"GPE\"` (geopolitical entity) for all matches.\n",
    "Add the component to the pipeline.\n",
    "Register the Span extension attribute `\"capital\"` with the getter `get_capital`.\n",
    "Process the text and print the entity text, entity label and entity capital for each entity span in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81abe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tutorial is based on v2 and current is v3. this part seems to be different from the tutorial sample codes\n",
    "'''\n",
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.language import Language\n",
    "\n",
    "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8a18b",
   "metadata": {},
   "source": [
    "### Processing streams\n",
    "\n",
    "In this exercise, you'll be using `nlp.pipe` for more efficient text processing. The `nlp` object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable `TEXTS`.\n",
    "\n",
    "Part 1\n",
    "- Rewrite the example to use `nlp.pipe`. Instead of iterating over the texts and processing them, iterate over the `doc` objects yielded by `nlp.pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d2d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open', 'BAD']\n",
      "['terrible']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "# for text in TEXTS:\n",
    "#     doc = nlp(text)\n",
    "#     print([token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d000e4",
   "metadata": {},
   "source": [
    "Part 2\n",
    "- Rewrite the example to use `nlp.pipe`. Don't forget to call `list()` around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3594f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "# docs = [nlp(text) for text in TEXTS]\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0e154",
   "metadata": {},
   "source": [
    "Part 3\n",
    "\n",
    "- Rewrite the example to use `nlp.pipe`. Don't forget to call `list()` around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8433ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "# patterns = [nlp(person) for person in people]\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e8f7b",
   "metadata": {},
   "source": [
    "### Processing data with context\n",
    "\n",
    "In this exercise, you'll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of `[text, context]` examples is available as the variable `DATA`. The texts are quotes from famous books, and the contexts dictionaries with the keys `\"author\"` and `\"book\"`.\n",
    "\n",
    "- Use the `set_extension` method to register the custom attributes `\"author\"` and `\"book\"` on the `Doc`, which default to `None`.\n",
    "- Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "- Overwrite the `doc._.book` and `doc._.author` with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c89a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " â€” 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " â€” 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " â€” 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " â€” 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " â€” '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " â€” 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n â€” '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b95b0f",
   "metadata": {},
   "source": [
    "### Selective processing\n",
    "\n",
    "In this exercise, you'll use the `nlp.make_doc` and `nlp.disable_pipes` methods to only run selected components when processing a text.\n",
    "\n",
    "Part 1\n",
    "- Rewrite the code to only tokenize the text using `nlp.make_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd768900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "# doc = nlp(text)\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2319815",
   "metadata": {},
   "source": [
    "Part 2\n",
    "- Disable the tagger and parser using the `nlp.disable_pipes` method.\n",
    "- Process the text and print all entities in the `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9867ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(ent.doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c32f7",
   "metadata": {},
   "source": [
    "### Creating training data(1)\n",
    "\n",
    "spaCy's rule-based `Matcher` is a great way to quickly create training data for named entity models. A list of sentences is available as the variable `TEXTS`. You can print it to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as `\"GADGET\"`.\n",
    "\n",
    "- Write a pattern for two tokens whose lowercase forms match `\"iphone\"` and `\"x\"`.\n",
    "- Write a pattern for two tokens: one token whose lowercase form matches `\"iphone\"` and a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612180ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "matcher.add works a bit different\n",
    "'''\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and check the result\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([doc[start:end] for match_id, start, end in matcher(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0691de",
   "metadata": {},
   "source": [
    "### Creating training data(2)\n",
    "\n",
    "Let's use the match patterns we've created in the previous exercise to bootstrap a set of training examples. A list of sentences is available as the variable `TEXTS`.\n",
    "\n",
    "- Create a doc object for each text using `nlp.pipe`.\n",
    "- Match on the `doc` and create a list of matched spans.\n",
    "- Get `(start character, end character, label)` tuples of matched spans.\n",
    "- Format each example as a tuple of the text and a dict, mapping `\"entities\"` to the entity tuples.\n",
    "- Append the example to `TRAINING_DATA` and inspect the printed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
